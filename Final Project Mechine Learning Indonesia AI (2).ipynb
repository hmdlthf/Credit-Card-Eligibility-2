{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Kelayakan Kartu Kredit: Faktor Penentu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Final Project Mechine Learning Indonesia AI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'RandomOverSampler' from 'imblearn.under_sampling' (D:\\LUTFI\\Anacnda\\envs\\streamlit\\lib\\site-packages\\imblearn\\under_sampling\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mseaborn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01msns\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mimblearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01munder_sampling\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m RandomOverSampler\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mimblearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mover_sampling\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SMOTE\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mimblearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpipeline\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Pipeline \u001b[38;5;28;01mas\u001b[39;00m ImbPipeline\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'RandomOverSampler' from 'imblearn.under_sampling' (D:\\LUTFI\\Anacnda\\envs\\streamlit\\lib\\site-packages\\imblearn\\under_sampling\\__init__.py)"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from imblearn.under_sampling import RandomOverSampler\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.svm import SVC\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, RandomizedSearchCV, StratifiedKFold\n",
    "from sklearn.metrics import confusion_matrix, classification_report, ConfusionMatrixDisplay, precision_score, recall_score, f1_score, accuracy_score, roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Membaca data\n",
    "data = pd.read_csv('dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Mengganti nama data menjadi df_data\n",
    "df_data = data.copy()\n",
    "\n",
    "#dimensi data\n",
    "print(df_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ringkasan dataset\n",
    "df_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*data integer : 12 kolom* <br>\n",
    "*data float : 3 kolom* <br>\n",
    "*data object : 5 kolom*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Melihat data yang duplikat \n",
    "df_data.duplicated().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*data tidak memiliki data yang duplikat*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Mengecek nilai kosong\n",
    "df_data.isnull().mean()*100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*data tidak memiliki data yang kosong*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Membuang kolom ID \n",
    "df_data.drop(columns = ['ID'], axis=1, inplace=True)\n",
    "df_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Mengecek jumlah kategori disetiap kolom kategori\n",
    "\n",
    "for i in df_data.columns:\n",
    "    if df_data[str(i)].dtype =='O':\n",
    "\n",
    "        print(i)\n",
    "        print(len(df_data[str(i)].drop_duplicates()))\n",
    "\n",
    "        if len(df_data[str(i)].drop_duplicates()) > 10:\n",
    "            print('Hapus Kolom = ', str(i))\n",
    "            df_data.drop(str(i), axis=1, inplace=True)\n",
    "\n",
    "            print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ringkasan Data\n",
    "df_data.describe(include = 'O')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Mengecek income_type future\n",
    "np.unique(df_data['Income_type'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(x='Income_type', data=df_data)\n",
    "plt.figure(figsize=(20,20))\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Mengecek Family_Status future\n",
    "np.unique(df_data['Family_status'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(x='Family_status', data=df_data)\n",
    "plt.figure(figsize=(20,20))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Melihat proporsi data target\n",
    "print(df_data['Target'].value_counts()/len(df_data['Target']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*data yang bernilai 0 867855* <br>\n",
    "*data yang bernilai 1 132145* <br>\n",
    "data ini menunjukkan ketimpangan antara 0 dan 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#visualisasi \n",
    "plt.figure(figsize=(8,6))\n",
    "sns.countplot(x = df_data['Target']);\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*sangat jomplang sekali data 0 dan 1*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Melihat kolom kategori yang bertipe object\n",
    "\n",
    "kategori = [var for var in df_data.columns if df_data[var].dtype == 'O']\n",
    "\n",
    "print(kategori)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dirubah menjadi tipe category\n",
    "for i in kategori:\n",
    "    df_data[i] = df_data[i].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ringkasan data kategory\n",
    "df_data.describe(include='category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = 'Target'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Mengelompokkan data kategory dan data numerik\n",
    "feature_kategory = [var for var in df_data.columns if df_data[var].dtype == 'category' and var!='Target']\n",
    "feature_numerik = [var for var in df_data.columns if df_data[var].dtype != 'category' and var!='Target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(feature_kategory)\n",
    "print(feature_numerik)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Melihat pairplot\n",
    "sns.pairplot(df_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Menentukan Variabel X dan Variabel y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Membagi data \n",
    "X = df_data.iloc[:, df_data.columns != 'Target']\n",
    "y = df_data['Target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spliting Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X,\n",
    "    y,\n",
    "    test_size = 0.2,\n",
    "    random_state = 0)\n",
    "\n",
    "# Melihat dimensi training set dan test set\n",
    "print((X_train.shape), (X_test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Balancing Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Melihat proporsi yang baru\n",
    "print(y_train.value_counts()/len(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Melihat dimensi data\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset index X_train dan X_test\n",
    "X_train = X_train.reset_index(drop=True)\n",
    "X_test = X_test.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Membuat Pipeline untuk preprocessing\n",
    "preprocessor_numerik = Pipeline([\n",
    "    ('imputasi', SimpleImputer(strategy='median')),\n",
    "    ('scaling', StandardScaler())\n",
    "])\n",
    "\n",
    "preprocessor_kategori = Pipeline([\n",
    "    ('imputasi', SimpleImputer(missing_values=np.nan, strategy='most_frequent')),\n",
    "    ('encoding', OneHotEncoder(drop='first', handle_unknown='ignore'))         \n",
    "])\n",
    "\n",
    "# Menggabungkan kedua pipeline di atas\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('preprocessing numerik', preprocessor_numerik, feature_numerik),\n",
    "    ('preprocessing kategori', preprocessor_kategori, feature_kategory)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model regresi logistik\n",
    "logreg = LogisticRegression(random_state=123, max_iter=1000)\n",
    "\n",
    "# Pipeline model regresi logistik\n",
    "pipe_logreg = ImbPipeline([\n",
    "    ('preprocessing', preprocessor),\n",
    "    ('smote', SMOTE(random_state=123)),\n",
    "    ('logreg', logreg)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fiting\n",
    "pipe_logreg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mencoba memprediksi test set\n",
    "pred_test = pipe_logreg.predict(X_test)\n",
    "\n",
    "# Merubahnya ke format Dataframe\n",
    "pred_test = pd.DataFrame(pred_test, columns=['Target'])\n",
    "\n",
    "# Melihat tmapilan y_pred\n",
    "pred_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confuse_log = confusion_matrix(y_test, pred_test)\n",
    "confuse_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Melihat confusion matrix dengan ConfusionMatrixDisplay()\n",
    "vis_cm = ConfusionMatrixDisplay(confusion_matrix = confuse_log,\n",
    "                                display_labels = pipe_logreg.classes_)\n",
    "vis_cm.plot(cmap=plt.cm.Blues)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Melihat ringkasan evaluasi\n",
    "print(classification_report(y_test,pred_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Menghitung metrics klasifikasi satu per satu\n",
    "print('Nilai akurasi: {:.2f}'.format(accuracy_score(y_test, pred_test)))\n",
    "print('Nilai presisi: {:.2f}'.format(precision_score(y_test, pred_test)))\n",
    "print('Nilai recall: {:.2f}'.format(recall_score(y_test, pred_test)))\n",
    "print('Nilai f1: {:.2f}'.format(f1_score(y_test, pred_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model regresi logistik\n",
    "logreg = LogisticRegression(max_iter=1000)\n",
    "\n",
    "# Pipeline model regresi logistik\n",
    "pipe_logreg = Pipeline([\n",
    "    ('preprocessing', preprocessor),\n",
    "    ('logreg', logreg)\n",
    "])\n",
    "\n",
    "# Hyperparameter tuning Logistic Regression\n",
    "param_logreg = {'logreg__penalty': ['l2'],\n",
    "                'logreg__C': [0.01, 0.09, 0.1, 0.125, 1, 1.5],\n",
    "                'logreg__solver' : ['liblinear', 'newton-cg', 'sag', 'saga', 'lbfgs']\n",
    "               }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model SVM\n",
    "svm = SVC(kernel = 'rbf', random_state = 0)\n",
    "\n",
    "# Pipeline model SVM\n",
    "pipe_svm = Pipeline([\n",
    "    ('preprocessing', preprocessor),\n",
    "    ('svm', svm)\n",
    "])\n",
    "\n",
    "# Hyperparameter tuning SVM\n",
    "param_svm = {'svm__C': [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 1],\n",
    "             'svm__gamma': [1, 0.1, 0.01, 0.001, 0.0001],\n",
    "             'svm__degree': [1, 2, 3, 4, 5],\n",
    "             'svm__kernel': ['linear', 'poly', 'rbf', 'sigmoid']\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model RF\n",
    "rf = RandomForestClassifier(random_state=0, n_jobs=-1)\n",
    "\n",
    "# Pipeline model RF\n",
    "pipe_rf = Pipeline([\n",
    "    ('preprocessing', preprocessor),\n",
    "    ('rf', rf),\n",
    "])\n",
    "\n",
    "# Hyperparameter tuning RF\n",
    "param_rf = {'rf__n_estimators': [200, 250, 300],\n",
    "            'rf__criterion': ['gini', 'entropy', 'log_loss'],\n",
    "            'rf__max_depth': [1, 2, 3, 4],\n",
    "            'rf__min_samples_split': [2, 3],\n",
    "            'rf__min_samples_leaf': [1, 2, 3],\n",
    "            'rf__warm_start': [True]\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model AdaBoost\n",
    "ada = AdaBoostClassifier(random_state=0)\n",
    "\n",
    "# Pipeline model AdaBoost\n",
    "pipe_ada = Pipeline([\n",
    "    ('preprocessing', preprocessor),\n",
    "    ('ada', ada),\n",
    "])\n",
    "\n",
    "# Hyperparameter tuning AdaBoost\n",
    "param_ada = {'ada__n_estimators': [10, 20, 30, 50, 70],\n",
    "             'ada__learning_rate': [0.3, 0.4, 0.5, 1, 1.1],\n",
    "             'ada__algorithm': ['SAMME', 'SAMME.R'],\n",
    "             }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model XGBoost\n",
    "xgb = XGBClassifier(random_state=0, n_jobs=-1)\n",
    "\n",
    "# Pipeline model XGBoost\n",
    "pipe_xgb = Pipeline([\n",
    "    ('preprocessing', preprocessor),\n",
    "    ('xgb', xgb),\n",
    "])\n",
    "\n",
    "param_xgb = {\n",
    "    'xgb__n_estimators': [200, 300, 400, 500, 600],\n",
    "    'xgb__max_depth': [3, 5, 7, 9, 11],\n",
    "    'xgb__learning_rate': [0.05, 0.1, 0.2, 0.3, 0.4],\n",
    "    'xgb__gamma': [0, 0.1, 0.2, 0.3, 0.4],\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline dan Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model training dengan cross validation\n",
    "daftar_model = [pipe_logreg, pipe_svm,  pipe_rf, pipe_ada, pipe_xgb]\n",
    "daftar_nama_model = ['logreg', 'svm', 'rf', 'ada', 'xgb']\n",
    "mean_akurasi = []\n",
    "mean_auc = []\n",
    "std_akurasi = []\n",
    "std_auc = []\n",
    "test_score_akurasi = []\n",
    "test_score_auc = []\n",
    "\n",
    "# Kita setting agar proses cross validasi dilakukan dengan sama rata (stratified) untuk pembagian kategori labelnya\n",
    "skf = StratifiedKFold(n_splits=3, random_state=0, shuffle=True)\n",
    "cv = skf\n",
    "\n",
    "# Looping untuk setiap model yang sudah disiapkan \n",
    "for i in daftar_model:\n",
    "    \n",
    "    # Melakukan cross validation dan menggunakan kriteria berdasarkan skor akurasi\n",
    "    cv_akurasi = cross_val_score(i, X_train, y_train, cv=cv, scoring='accuracy', verbose=1, n_jobs=-1)  # kita kalikan dengan -1 karena scoring menggunakan nilai negatif\n",
    "    cv_auc = cross_val_score(i, X_train, y_train, cv=cv, scoring='roc_auc', verbose=1, n_jobs=-1)\n",
    "    \n",
    "    # Menghitung nilai rata-rata akurasi dan menambahkannya ke variabel mean_akurasi\n",
    "    mean_akurasi.append(round(cv_akurasi.mean(),2))   # round(nilai,2) untuk membulatkan nilai 2 angka di belakang koma\n",
    "    mean_auc.append(round(cv_auc.mean(),2))\n",
    "\n",
    "    # Menghitung nilai standar deviasi akurasi dan menambahkannya ke variabel std_akurasi\n",
    "    std_akurasi.append(round(cv_akurasi.std(),2))\n",
    "    std_auc.append(round(cv_auc.std(),2))\n",
    "    \n",
    "    # Melakukan fitting training set kemudian melakukan prediksi  di test set\n",
    "    i.fit(X_train, y_train)\n",
    "    i_predict = i.predict(X_test)\n",
    "    \n",
    "    # Menghitung nilai rata-rata akurasi di test set dan menambahkannya ke variabel test_score_akurasi\n",
    "    test_score_akurasi.append(round(accuracy_score(y_test, i_predict),2))\n",
    "    test_score_auc.append(round(roc_auc_score(y_test, i_predict),2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Membuat DataFrame\n",
    "cv_akurasi = pd.DataFrame({'model':daftar_nama_model, 'Train_Mean':mean_akurasi, 'std':std_akurasi, 'Test_Score':test_score_akurasi})\n",
    "cv_auc = pd.DataFrame({'model':daftar_nama_model, 'Train_Mean':mean_auc, 'std':std_auc, 'Test_Score':test_score_auc})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Melihat hasil cross validation berdasarkan akurasi\n",
    "cv_akurasi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Melihat hasil cross validation berdasarkan akurasi\n",
    "cv_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mengurutkan skor dari kecil ke besar berdasarkan skor akurasi\n",
    "cv_akurasi_urut = cv_akurasi.sort_values(by=['Train_Mean', 'Test_Score'], ascending=False,  ignore_index=True)\n",
    "cv_akurasi_urut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mengurutkan skor dari kecil ke besar berdasarkan skor AUC\n",
    "cv_auc_urut = cv_auc.sort_values(by=['Train_Mean', 'Test_Score'], ascending=False,  ignore_index=True)\n",
    "cv_auc_urut"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model training dengan RandomSearchCV\n",
    "daftar_model = [pipe_logreg, pipe_svm,  pipe_rf, pipe_ada, pipe_xgb]\n",
    "daftar_nama_model = ['logreg', 'svm', 'rf', 'ada', 'xgb']\n",
    "daftar_param_model = [param_logreg,  param_svm, param_rf, param_ada, param_xgb]\n",
    "\n",
    "# Nilai yang akan diisikan\n",
    "akurasi_tuning = []\n",
    "akurasi_tuning_test = []\n",
    "auc_tuning = []\n",
    "auc_tuning_test = []\n",
    "best_param = []\n",
    "best_estimator = []\n",
    "\n",
    "for i in range(len(daftar_model)):\n",
    "    \n",
    "    # Menjalankan RandomizedSearchCV\n",
    "    model_random_cv = RandomizedSearchCV(\n",
    "        daftar_model[i],\n",
    "        daftar_param_model[i],\n",
    "        cv=5,\n",
    "        scoring='accuracy',  # nilai akurasi digunakan sebagai dasar penentuan peringkat parameter terbaik dan parameter scoring ini bisa diganti yang lain\n",
    "        verbose=1,\n",
    "        n_jobs=-1\n",
    "        )\n",
    "    \n",
    "    # Fitting ke training set\n",
    "    model_random_cv.fit(X_train,y_train)\n",
    "    \n",
    "    # Mencoba memprediksi training dan test set setelah fitting di training set, kemudian dikemas dalam format DataFrame\n",
    "    pred_train = pd.DataFrame(model_random_cv.predict(X_train), columns=['Eligible'])\n",
    "    pred_test = pd.DataFrame(model_random_cv.predict(X_test), columns=['Eligible'])\n",
    "\n",
    "    # Mencatat skor MAE training dan test set\n",
    "    akurasi_tuning.append(accuracy_score(y_train, pred_train))\n",
    "    akurasi_tuning_test.append(accuracy_score(y_test, pred_test))\n",
    "\n",
    "    # Mencatat skor R2 training dan test set\n",
    "    auc_tuning.append(roc_auc_score(y_train, pred_train))\n",
    "    auc_tuning_test.append(roc_auc_score(y_test, pred_test))   \n",
    "    \n",
    "    # Mencatat parameter terbaik di setiap model\n",
    "    best_param.append(model_random_cv.best_params_)\n",
    "    \n",
    "    # Merekam settingan modelnya\n",
    "    best_estimator.append(model_random_cv.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model training dengan GridSearchCV\n",
    "daftar_model = [pipe_logreg, pipe_svm,  pipe_rf, pipe_ada, pipe_xgb]\n",
    "daftar_nama_model = ['logreg', 'svm', 'rf', 'ada', 'xgb']\n",
    "daftar_param_model = [param_logreg,  param_svm, param_rf, param_ada, param_xgb]\n",
    "\n",
    "# Nilai yang akan diisikan\n",
    "akurasi_tuning = []\n",
    "akurasi_tuning_test = []\n",
    "auc_tuning = []\n",
    "auc_tuning_test = []\n",
    "best_param = []\n",
    "best_estimator = []\n",
    "\n",
    "for i in range(len(daftar_model)):\n",
    "    \n",
    "    # Menjalankan GridSearchCV\n",
    "    model_grid_cv = GridSearchCV(\n",
    "        daftar_model[i],\n",
    "        daftar_param_model[i],\n",
    "        cv=3,\n",
    "        scoring='accuracy',  # nilai akurasi digunakan sebagai dasar penentuan peringkat parameter terbaik dan parameter scoring ini bisa diganti yang lain\n",
    "        verbose=1,\n",
    "        n_jobs=-1\n",
    "        )\n",
    "    \n",
    "    # Fitting ke training set\n",
    "    model_grid_cv.fit(X_train,y_train)\n",
    "    \n",
    "    # Mencoba memprediksi training dan test set setelah fitting di training set, kemudian dikemas dalam format DataFrame\n",
    "    pred_train = pd.DataFrame(model_grid_cv.predict(X_train), columns=['Eligible'])\n",
    "    pred_test = pd.DataFrame(model_grid_cv.predict(X_test), columns=['Eligible'])\n",
    "\n",
    "    # Mencatat skor MAE training dan test set \n",
    "    akurasi_tuning.append(accuracy_score(y_train, pred_train))\n",
    "    akurasi_tuning_test.append(accuracy_score(y_test, pred_test))\n",
    "\n",
    "    # Mencatat skor R2 training dan test set\n",
    "    auc_tuning.append(roc_auc_score(y_train, pred_train))\n",
    "    auc_tuning_test.append(roc_auc_score(y_test, pred_test))   \n",
    "    \n",
    "    # Mencatat parameter terbaik di setiap model\n",
    "    best_param.append(model_grid_cv.best_params_)\n",
    "    \n",
    "    # Merekam settingan modelnya\n",
    "    best_estimator.append(model_grid_cv.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Membuat DataFrame sekaligus kita urutkan\n",
    "grid_akurasi = pd.DataFrame({'model':daftar_nama_model, 'Training':akurasi_tuning, 'Testing':akurasi_tuning_test})\n",
    "grid_akurasi_urut = grid_akurasi.sort_values(by='Testing', ascending=False,  ignore_index=True)\n",
    "\n",
    "grid_auc = pd.DataFrame({'model':daftar_nama_model, 'Training':auc_tuning, 'Testing':auc_tuning_test})\n",
    "grid_auc_urut = grid_auc.sort_values(by='Testing', ascending=False, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Melihat performa tuning berdasarkan MAE\n",
    "grid_akurasi_urut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Melihat performa tuning berdasarkan R2\n",
    "grid_auc_urut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameter terbaik dari model terbaik\n",
    "grid_param = pd.DataFrame({'model':daftar_nama_model, 'Param':best_param})\n",
    "grid_param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estimator terbaik dari model terbaik\n",
    "grid_estimator = pd.DataFrame({'model':daftar_nama_model, 'Param':best_estimator})\n",
    "grid_estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kita gabungkan semua sebagai 'model_best'\n",
    "model_best = pd.DataFrame({'model':daftar_nama_model, 'Param':best_estimator, 'Testing':akurasi_tuning_test})\n",
    "model_best = model_best.sort_values(by='Testing', ascending=False, ignore_index=True)\n",
    "model_best"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### model Terbaik"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting ke dataset utuh\n",
    "model_final = model_best['Param'][2].fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Melihat spesifikasi model_final\n",
    "model_final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Melihat *feature importance* dari model terbaik."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Menghitung importances dari pipeline adaboost\n",
    "# importances = model_final[1].feature_importances_\n",
    "# importances = pd.Series(importances, index=model_final['preprocessing'].get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Melakukan plotting\n",
    "# fig, ax = plt.subplots(figsize = (8,10))\n",
    "# importances.sort_values(ascending=False).plot.bar(ax=ax)\n",
    "# ax.set_title(\"Plot Feature Importance Setiap Feature\")\n",
    "# ax.set_ylabel(\"Mean decrease in impurity\")\n",
    "# fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importances = model_final[1].feature_importances_\n",
    "# importances = pd.Series(importances, index=model_final['preprocessing'].get_feature_names_out())\n",
    "\n",
    "# # Filter importances to keep only non-zero values\n",
    "# non_zero_importances = importances[importances > 0]\n",
    "\n",
    "# # Create a new DataFrame from the filtered series\n",
    "# important_features_df = pd.DataFrame({\n",
    "#     'Feature': non_zero_importances.index,\n",
    "#     'Importance': non_zero_importances.values\n",
    "# })\n",
    "\n",
    "# important_features_df['Feature'] = important_features_df['Feature'].str.replace('preprocessing numerik__', '', regex=False)\n",
    "# important_features_df['Feature'] = important_features_df['Feature'].str.replace('preprocessing kategori__', '', regex=False)\n",
    "\n",
    "# # Optionally, sort the DataFrame by importance\n",
    "# important_features_df = important_features_df.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "# # Display the new DataFrame\n",
    "# important_features_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transformed_data = preprocessor.fit_transform(X_train)\n",
    "\n",
    "# encoded_cat_columns = preprocessor.named_transformers_['preprocessing kategori'].get_feature_names_out(feature_kategory)\n",
    "\n",
    "# transformed_categorical_df = pd.DataFrame(transformed_data[:, len(feature_numerik):], columns=encoded_cat_columns)\n",
    "\n",
    "# transformed_df = pd.concat([pd.DataFrame(transformed_data[:, :len(feature_numerik)], columns=feature_numerik), transformed_categorical_df], axis=1)\n",
    "\n",
    "# df_baru = transformed_df[important_features_df['Feature']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Memprediksi df_test\n",
    "hasil = model_final.predict(df_data)\n",
    "hasil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.read_csv('dataset.csv')\n",
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Memprediksi df_test\n",
    "hasil = model_final.predict(df_test)\n",
    "hasil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Keputusan\n",
    "# for i in range(len(hasil)):\n",
    "#     if hasil[i] == 1:\n",
    "#         print('Data pelanggan',df_test['ID'][i],'= Layak')\n",
    "#     else:\n",
    "#         print('Data pelanggan',df_test['ID'][i],'= Tidak Layak')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Library untuk menyimpan model\n",
    "import pickle\n",
    "\n",
    "# Menyimpan model dengan nama 'model_regresi_terbaik.pkl'\n",
    "pickle.dump(model_final, open('model_creditcard.pkl', 'wb'))\n",
    "\n",
    "# Load lagi model yang sudah disimpan dan buka sebagai 'best_model'\n",
    "best_model = pickle.load(open('model_creditcard.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mencoba melakukan prediksi X_test\n",
    "prediksi = best_model.predict(df_test)\n",
    "\n",
    "# Melihat hasil prediksi\n",
    "prediksi"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
